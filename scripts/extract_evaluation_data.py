#!/usr/bin/env python
"""
Phase 5-B: í‰ê°€ íŒŒì¸íŠœë‹ ë°ì´í„° ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ í‰ê°€ ì ìˆ˜, ë¶„ì„, ë£¨ë¸Œë¦­ì„ ì¶”ì¶œí•˜ì—¬ í‰ê°€ LLM í’ˆì§ˆ ê°œì„ ì— í™œìš©í•©ë‹ˆë‹¤.

ì¶œë ¥ íŒŒì¼:
- evaluation_data.jsonl      : ì „ì²´ í‰ê°€ ë°ì´í„°
- evaluation_cleaned.jsonl   : ì •ì œëœ ë°ì´í„° (score, analysis í•„ìˆ˜)
- eval_high_score.jsonl       : ê³ ì  í”„ë¡¬í”„íŠ¸ (70+)
- eval_medium_score.jsonl    : ì¤‘ì  í”„ë¡¬í”„íŠ¸ (40-69)
- eval_low_score.jsonl       : ì €ì  í”„ë¡¬í”„íŠ¸ (0-39)
- evaluation_examples.json   : Few-shot ì˜ˆì‹œ (ì˜ë„ë³„/ì ìˆ˜ëŒ€ë³„)

ì‚¬ìš©ë²•:
    python scripts/extract_evaluation_data.py
    python scripts/extract_evaluation_data.py --output-dir ./custom_output
"""

import argparse
import json
import os
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(project_root / ".env")


def get_db_config() -> dict:
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ DB ì„¤ì • ì½ê¸°"""
    return {
        "host": os.getenv("POSTGRES_HOST", "localhost"),
        "port": int(os.getenv("POSTGRES_PORT", 5435)),
        "user": os.getenv("POSTGRES_USER", "postgres"),
        "password": os.getenv("POSTGRES_PASSWORD", "postgres"),
        "database": os.getenv("POSTGRES_DB", "ai_vibe_coding_test"),
    }


def connect_db():
    """PostgreSQL ì—°ê²°"""
    config = get_db_config()
    print(f"[INFO] DB ì—°ê²° ì¤‘: {config['host']}:{config['port']}/{config['database']}")
    conn = psycopg2.connect(
        host=config["host"],
        port=config["port"],
        user=config["user"],
        password=config["password"],
        database=config["database"],
    )
    return conn


# SQL ì¿¼ë¦¬: í‰ê°€ ë°ì´í„° ì¶”ì¶œ
EXTRACT_EVALUATION_DATA_SQL = """
SET search_path TO ai_vibe_coding_test;

SELECT 
    pe.id,
    pe.session_id,
    pe.turn,
    pm.content AS user_prompt,
    pe.details,
    pe.created_at
FROM ai_vibe_coding_test.prompt_evaluations pe
JOIN ai_vibe_coding_test.prompt_messages pm 
    ON pe.session_id = pm.session_id 
    AND pe.turn = pm.turn
WHERE UPPER(pm.role) = 'USER'
    AND pe.evaluation_type = 'TURN_EVAL'
    AND pe.details->>'score' IS NOT NULL
ORDER BY pe.session_id, pe.turn;
"""


def parse_details(details: dict) -> dict[str, Any]:
    """details JSONB í•„ë“œ íŒŒì‹±"""
    result = {}
    
    # ê¸°ë³¸ í•„ë“œ
    result["score"] = _safe_float(details.get("score"))
    result["analysis"] = details.get("analysis", "")
    result["intent"] = details.get("intent")
    result["intent_confidence"] = _safe_float(details.get("intent_confidence"))
    result["is_guardrail_failed"] = _safe_bool(details.get("is_guardrail_failed"))
    
    # ë£¨ë¸Œë¦­ íŒŒì‹± (ë°°ì—´ ë˜ëŠ” ê°ì²´ í˜•íƒœ)
    rubrics_raw = details.get("rubrics", [])
    result["rubrics"] = {}
    
    if isinstance(rubrics_raw, list):
        # ë°°ì—´ í˜•íƒœ: [{"name": "clarity", "score": 40.0, "reasoning": "..."}, ...]
        for rubric in rubrics_raw:
            if isinstance(rubric, dict):
                name = rubric.get("name")
                if name:
                    result["rubrics"][name] = {
                        "score": _safe_float(rubric.get("score")),
                        "reasoning": rubric.get("reasoning", "")
                    }
    elif isinstance(rubrics_raw, dict):
        # ê°ì²´ í˜•íƒœ: {"clarity": {"score": 40.0, "reasoning": "..."}, ...}
        result["rubrics"] = {
            name: {
                "score": _safe_float(value.get("score") if isinstance(value, dict) else None),
                "reasoning": value.get("reasoning", "") if isinstance(value, dict) else ""
            }
            for name, value in rubrics_raw.items()
        }
    
    # ê°€ì¤‘ì¹˜ ì •ë³´
    result["weights"] = details.get("weights", {})
    
    return result


def _safe_float(value: Any) -> float | None:
    """ì•ˆì „í•˜ê²Œ float ë³€í™˜"""
    if value is None:
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        return None


def _safe_bool(value: Any) -> bool:
    """ì•ˆì „í•˜ê²Œ bool ë³€í™˜"""
    if value is None:
        return False
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        return value.lower() in ("true", "1", "yes")
    return bool(value)


def extract_evaluation_data(conn) -> list[dict[str, Any]]:
    """DBì—ì„œ í‰ê°€ ë°ì´í„° ì¶”ì¶œ"""
    print("[INFO] í‰ê°€ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute(EXTRACT_EVALUATION_DATA_SQL)
        rows = cur.fetchall()
    
    results = []
    for row in rows:
        details = row.get("details", {})
        if not isinstance(details, dict):
            continue
        
        parsed = parse_details(details)
        
        # ì¶œë ¥ ë ˆì½”ë“œ êµ¬ì„±
        record = {
            "id": f"eval_{row['session_id']}_{row['turn']}",
            "user_prompt": row["user_prompt"],
            "intent": parsed.get("intent"),
            "intent_confidence": parsed.get("intent_confidence"),
            "score": parsed.get("score"),
            "rubrics": parsed.get("rubrics", {}),
            "weights": parsed.get("weights", {}),
            "analysis": parsed.get("analysis", ""),
            "is_guardrail_failed": parsed.get("is_guardrail_failed", False),
            "metadata": {
                "session_id": row["session_id"],
                "turn": row["turn"],
                "created_at": row["created_at"].isoformat() if row.get("created_at") else None,
            }
        }
        results.append(record)
    
    print(f"[INFO] ì´ {len(results)}ê°œì˜ í‰ê°€ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return results


def clean_data(evaluations: list[dict]) -> list[dict]:
    """ë°ì´í„° ì •ì œ: scoreê°€ NULLì´ê±°ë‚˜ analysisê°€ ë¹„ì–´ìˆëŠ” ë°ì´í„° ì œì™¸"""
    cleaned = []
    for eval_data in evaluations:
        score = eval_data.get("score")
        analysis = eval_data.get("analysis", "").strip()
        
        if score is not None and analysis:
            cleaned.append(eval_data)
    
    print(f"[INFO] ì •ì œ ê²°ê³¼: {len(cleaned)}ê°œ (ì „ì²´ {len(evaluations)}ê°œ ì¤‘)")
    return cleaned


def categorize_by_score(evaluations: list[dict]) -> dict[str, list[dict]]:
    """ì ìˆ˜ëŒ€ë³„ë¡œ ë¶„ë¥˜"""
    by_score = {
        "high": [],      # 70+
        "medium": [],    # 40-69
        "low": [],       # 0-39
    }
    
    for eval_data in evaluations:
        score = eval_data.get("score")
        if score is None:
            continue
        elif score >= 70:
            by_score["high"].append(eval_data)
        elif score >= 40:
            by_score["medium"].append(eval_data)
        else:
            by_score["low"].append(eval_data)
    
    return by_score


def categorize_by_intent(evaluations: list[dict]) -> dict[str, list[dict]]:
    """ì˜ë„ë³„ë¡œ ë¶„ë¥˜"""
    by_intent = defaultdict(list)
    for eval_data in evaluations:
        intent = eval_data.get("intent") or "UNKNOWN"
        by_intent[intent].append(eval_data)
    return dict(by_intent)


def select_few_shot_examples(evaluations: list[dict]) -> dict:
    """Few-shot ì˜ˆì‹œ ì„ ì • (ì˜ë„ë³„/ì ìˆ˜ëŒ€ë³„ ëŒ€í‘œ ì˜ˆì‹œ)"""
    examples = {
        "by_intent": {},
        "by_score": {
            "high": [],
            "medium": [],
            "low": []
        },
        "best_examples": []  # reasoningì´ ëª…í™•í•˜ê³  ì ìˆ˜ê°€ ë†’ì€ ì˜ˆì‹œ
    }
    
    # ì˜ë„ë³„ ì˜ˆì‹œ ì„ ì • (ê° ì˜ë„ë‹¹ ìµœëŒ€ 5ê°œ, reasoningì´ ëª…í™•í•œ ê²ƒ ìš°ì„ )
    by_intent = categorize_by_intent(evaluations)
    for intent, items in by_intent.items():
        # reasoning ê¸¸ì´ì™€ ì ìˆ˜ë¡œ ì •ë ¬
        sorted_items = sorted(
            items,
            key=lambda x: (
                len(x.get("analysis", "")),  # analysis ê¸¸ì´ (ëª…í™•í•œ reasoning)
                x.get("score", 0)  # ì ìˆ˜
            ),
            reverse=True
        )
        examples["by_intent"][intent] = sorted_items[:5]
    
    # ì ìˆ˜ëŒ€ë³„ ì˜ˆì‹œ ì„ ì •
    by_score = categorize_by_score(evaluations)
    for score_level, items in by_score.items():
        # analysisê°€ ëª…í™•í•œ ê²ƒ ìš°ì„ 
        sorted_items = sorted(
            items,
            key=lambda x: len(x.get("analysis", "")),
            reverse=True
        )
        examples["by_score"][score_level] = sorted_items[:5]
    
    # ìµœê³  ì˜ˆì‹œ (ì ìˆ˜ 70+ ì´ê³  analysisê°€ ëª…í™•í•œ ê²ƒ)
    high_score = by_score["high"]
    sorted_high = sorted(
        high_score,
        key=lambda x: (len(x.get("analysis", "")), x.get("score", 0)),
        reverse=True
    )
    examples["best_examples"] = sorted_high[:10]
    
    return examples


def save_jsonl(data: list[dict], filepath: Path):
    """JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False, default=str) + "\n")
    print(f"[INFO] ì €ì¥ ì™„ë£Œ: {filepath} ({len(data)}ê°œ ë ˆì½”ë“œ)")


def save_json(data: dict, filepath: Path):
    """JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)
    print(f"[INFO] ì €ì¥ ì™„ë£Œ: {filepath}")


def print_statistics(evaluations: list[dict], cleaned: list[dict]):
    """í†µê³„ ì •ë³´ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ì¶”ì¶œ ê²°ê³¼ í†µê³„")
    print("=" * 60)
    
    print(f"\nğŸ“Œ ì „ì²´ í‰ê°€ ë°ì´í„°: {len(evaluations)}ê°œ")
    print(f"   - ì •ì œëœ ë°ì´í„°: {len(cleaned)}ê°œ")
    
    # ì ìˆ˜ëŒ€ë³„ ë¶„í¬
    by_score = categorize_by_score(cleaned)
    print(f"\nğŸ“Œ ì ìˆ˜ëŒ€ë³„ ë¶„í¬:")
    print(f"   - ê³ ì  (70+): {len(by_score['high'])}ê°œ")
    print(f"   - ì¤‘ì  (40-69): {len(by_score['medium'])}ê°œ")
    print(f"   - ì €ì  (0-39): {len(by_score['low'])}ê°œ")
    
    # ì˜ë„ë³„ ë¶„í¬
    by_intent = categorize_by_intent(cleaned)
    print(f"\nğŸ“Œ ì˜ë„ë³„ ë¶„í¬:")
    for intent, items in sorted(by_intent.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"   - {intent}: {len(items)}ê°œ")
    
    # ì ìˆ˜ í†µê³„
    scores = [e.get("score") for e in cleaned if e.get("score") is not None]
    if scores:
        print(f"\nğŸ“Œ ì ìˆ˜ í†µê³„:")
        print(f"   - í‰ê· : {sum(scores) / len(scores):.2f}")
        print(f"   - ìµœê³ : {max(scores):.2f}")
        print(f"   - ìµœì €: {min(scores):.2f}")
    
    print("\n" + "=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="Phase 5-B: í‰ê°€ íŒŒì¸íŠœë‹ ë°ì´í„° ì¶”ì¶œ"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=".maestro/data/finetuning/phase5b_evaluation",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: .maestro/data/finetuning/phase5b_evaluation)"
    )
    args = parser.parse_args()
    
    output_dir = Path(project_root) / args.output_dir
    
    print("=" * 60)
    print("ğŸš€ Phase 5-B: í‰ê°€ íŒŒì¸íŠœë‹ ë°ì´í„° ì¶”ì¶œ")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print("=" * 60)
    
    try:
        # DB ì—°ê²°
        conn = connect_db()
        
        # ë°ì´í„° ì¶”ì¶œ
        evaluations = extract_evaluation_data(conn)
        
        if not evaluations:
            print("[WARN] ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. DBì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return
        
        # ë°ì´í„° ì •ì œ
        cleaned = clean_data(evaluations)
        
        if not cleaned:
            print("[WARN] ì •ì œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. scoreì™€ analysisê°€ ìˆëŠ” ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # í†µê³„ ì¶œë ¥
        print_statistics(evaluations, cleaned)
        
        # ë¶„ë¥˜
        by_score = categorize_by_score(cleaned)
        
        # íŒŒì¼ ì €ì¥
        print("\nğŸ“ íŒŒì¼ ì €ì¥ ì¤‘...")
        
        # 1. ì „ì²´ í‰ê°€ ë°ì´í„°
        save_jsonl(evaluations, output_dir / "evaluation_data.jsonl")
        
        # 2. ì •ì œëœ ë°ì´í„°
        save_jsonl(cleaned, output_dir / "evaluation_cleaned.jsonl")
        
        # 3. ì ìˆ˜ëŒ€ë³„ ë¶„ë¥˜
        save_jsonl(by_score["high"], output_dir / "eval_high_score.jsonl")
        save_jsonl(by_score["medium"], output_dir / "eval_medium_score.jsonl")
        save_jsonl(by_score["low"], output_dir / "eval_low_score.jsonl")
        
        # 4. Few-shot ì˜ˆì‹œ
        examples = select_few_shot_examples(cleaned)
        save_json(examples, output_dir / "evaluation_examples.json")
        
        print("\nâœ… Phase 5-B ì™„ë£Œ!")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        
        conn.close()
        
    except psycopg2.Error as e:
        print(f"[ERROR] DB ì˜¤ë¥˜: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"[ERROR] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

# 정답 코드 일부 제공 정책 (평가 도구 관점)

## 📋 질문

**"정답 코드를 일부 제공하는 건 어떻다고 생각해?"**
- 사용자가 정답 코드로 올린 부분의 일부만 제공
- LLM 사용의 평가 도구로써 툴을 쓴다고 했을 때

---

## 🎯 평가 도구로서의 관점

### 현재 시스템의 목적

1. **프롬프트 엔지니어링 능력 평가**
   - 사용자가 LLM을 효과적으로 활용하는 능력 측정
   - 명확성, 문제 적절성, 예시, 규칙, 문맥 등 5가지 기준 평가

2. **학습 목적**
   - 사용자가 스스로 문제를 해결하는 과정 평가
   - 문제 분해, 전략 수립, 피드백 수용성 등 평가

3. **공정성**
   - 모든 사용자에게 동일한 조건 제공
   - 평가의 신뢰성과 일관성 유지

---

## 🤔 정답 코드 일부 제공에 대한 분석

### ✅ **지지하는 관점**

#### 1. **학습 효과 향상**
- **점진적 학습**: 일부 코드를 보면서 나머지 부분을 스스로 완성하는 과정에서 학습 효과 증가
- **구체적 피드백**: 추상적인 설명보다 실제 코드를 보는 것이 더 명확한 이해 제공
- **패턴 학습**: 올바른 코드 패턴을 직접 보면서 학습 가능

#### 2. **평가의 현실성**
- **실제 개발 환경 반영**: 실제 개발에서는 기존 코드베이스나 레퍼런스를 참고하는 것이 일반적
- **프롬프트 엔지니어링 능력**: "어떤 부분을 요청하는가"도 중요한 평가 요소
  - 예: "이 부분만 수정해줘", "이 함수만 구현해줘" 등

#### 3. **평가 기준의 확장**
- **문맥 활용 능력**: 기존 코드를 문맥으로 활용하는 능력 평가 가능
- **문제 분해 능력**: 전체 문제를 부분으로 나누어 접근하는 능력 평가

---

### ❌ **반대하는 관점**

#### 1. **평가의 공정성 훼손**
- **정보 비대칭**: 정답 코드 일부를 본 사용자와 보지 않은 사용자 간 불공정
- **점수 왜곡**: 코드 일부 제공으로 인해 실제 프롬프트 품질과 점수 간 불일치 발생 가능
- **평가 기준 혼란**: "프롬프트 품질" vs "코드 활용 능력" 평가 목적 혼재

#### 2. **학습 목적 훼손**
- **의존성 증가**: 코드 일부를 보면 나머지 부분을 스스로 생각하는 능력 저하
- **문제 해결 능력 평가 어려움**: 전체 문제를 처음부터 해결하는 능력 측정 불가
- **학습 곡선 왜곡**: 실제 학습 진전을 정확히 측정하기 어려움

#### 3. **현재 시스템과의 충돌**
- **가드레일 시스템**: 현재 시스템은 "정답 코드" 요청을 차단하도록 설계됨
- **평가 일관성**: 모든 사용자에게 동일한 조건을 제공해야 평가가 공정함

---

## 💡 **제안: 조건부 허용 정책**

### 시나리오 1: **"코드 수정/최적화" 의도로 명확히 구분**

```python
# 허용 가능한 경우
"이 함수의 시간 복잡도를 O(N log N)으로 개선해줘"
"이 부분만 비트마스킹으로 바꿔줘"
"이 코드에서 버그를 찾아줘"

# 차단해야 하는 경우
"정답 코드를 보여줘"
"전체 코드를 완성해줘"
```

**평가 기준 조정**:
- **문맥 (Context)**: 기존 코드를 문맥으로 활용하는 능력 평가 (점수 가중)
- **문제 분해 (Problem Decomposition)**: 전체 문제를 부분으로 나누는 능력 평가

---

### 시나리오 2: **"학습 단계별 차등 평가"**

#### 단계 1: 초기 학습 단계 (턴 1-3)
- ❌ 정답 코드 일부 제공 차단
- 목적: 기본 문제 해결 능력 평가

#### 단계 2: 중간 학습 단계 (턴 4-6)
- ✅ 조건부 허용: "코드 수정/최적화" 의도만 허용
- 목적: 문제 분해 및 부분 해결 능력 평가

#### 단계 3: 고급 학습 단계 (턴 7+)
- ✅ 허용: 모든 의도 허용 (단, 평가 기준 강화)
- 목적: 복잡한 문제 해결 및 코드 통합 능력 평가

**평가 기준 조정**:
- 초기 단계: 기본 프롬프트 품질 평가 (현재 기준)
- 중간 단계: 문제 분해 능력 + 문맥 활용 능력 평가
- 고급 단계: 코드 통합 능력 + 최적화 능력 평가

---

### 시나리오 3: **"의도별 차등 평가"**

#### 허용 의도
- ✅ **OPTIMIZATION**: 기존 코드 최적화
- ✅ **DEBUGGING**: 버그 수정
- ✅ **TEST_CASE**: 테스트 케이스 작성

#### 제한 의도
- ⚠️ **GENERATION**: 새로운 코드 생성 (부분 코드 제공 시 평가 기준 강화)
- ❌ **SYSTEM_PROMPT**: 시스템 프롬프트 설정 (코드 제공 불필요)

**평가 기준 조정**:
- 허용 의도: 문맥 활용 능력 평가 추가
- 제한 의도: 기존 평가 기준 유지

---

## 🎯 **최종 권장사항**

### **옵션 A: 현재 정책 유지 (권장)**

**이유**:
1. **평가의 공정성**: 모든 사용자에게 동일한 조건 제공
2. **평가 목적 명확성**: 프롬프트 품질 평가에 집중
3. **시스템 일관성**: 가드레일 시스템과의 일관성 유지

**대안**:
- 정답 코드 일부 제공을 원하는 경우, **별도의 "코드 리뷰 모드"** 제공
- 평가 모드와 학습 모드를 분리하여 운영

---

### **옵션 B: 조건부 허용 (신중한 접근)**

**조건**:
1. **의도 명확화**: "코드 수정/최적화" 의도로만 허용
2. **평가 기준 조정**: 문맥 활용 능력 평가 추가
3. **가드레일 강화**: "정답 코드 전체" 요청은 여전히 차단

**평가 기준 수정 예시**:
```python
# 기존 평가 기준
5. 문맥 (Context): 이전 대화나 배경 지식을 적절히 활용했는가?

# 수정된 평가 기준 (코드 일부 제공 시)
5. 문맥 (Context): 
   - 이전 대화나 배경 지식을 적절히 활용했는가?
   - 기존 코드를 문맥으로 활용하여 수정 요청을 구체적으로 명시했는가?
   - 전체 코드 대신 필요한 부분만 요청했는가?
```

---

## 📊 **평가 도구로서의 결론**

### **핵심 원칙**

1. **평가의 공정성 우선**
   - 모든 사용자에게 동일한 조건 제공
   - 평가 목적과 일치하는 정책 수립

2. **학습 목적 명확화**
   - 평가 모드 vs 학습 모드 분리
   - 각 모드별 목적과 기준 명확히 정의

3. **유연한 정책 수립**
   - 의도별, 단계별 차등 평가
   - 평가 기준 동적 조정

### **권장 방향**

**단기**: 현재 정책 유지 (정답 코드 일부 제공 차단)
- 평가의 공정성과 일관성 유지
- 평가 목적 명확성 유지

**중장기**: 조건부 허용 정책 검토
- 의도별 차등 평가 도입
- 평가 기준 확장 (문맥 활용 능력 등)
- 사용자 피드백 수집 및 분석

---

## 🔗 **관련 문서**

- `docs/Evaluation_System_Architecture.md`: 평가 시스템 전체 구조
- `docs/Prompt_Engineering_Guide.md`: 프롬프트 작성 가이드
- `docs/Guardrail_Implementation_Strategy.md`: 가드레일 시스템 구현 전략


# LLM Factory Pattern ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì—¬ëŸ¬ LLM íƒ€ì…ì„ ì§€ì›í•˜ê³ , ë…¸ë“œë³„ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” Factory Pattern êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.

**ëª©ì **:
- ì—¬ëŸ¬ LLM íƒ€ì… ì§€ì› (Gemini, OpenAI, Anthropic ë“±)
- ë…¸ë“œë³„ë¡œ ë‹¤ë¥¸ LLM ì„¤ì • ì‚¬ìš© ê°€ëŠ¥
- ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš© (ì‹±ê¸€í†¤ íŒ¨í„´)
- í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°

---

## ğŸ—ï¸ êµ¬ì¡°

### íŒŒì¼ ìœ„ì¹˜
- **êµ¬í˜„**: `app/domain/langgraph/utils/llm_factory.py`
- **ìƒíƒœ**: êµ¬í˜„ ì™„ë£Œ, ì„ íƒì  ì‚¬ìš© ê°€ëŠ¥

### í˜„ì¬ ìƒíƒœ
- âœ… Factory Pattern êµ¬í˜„ ì™„ë£Œ
- âš ï¸ ì•„ì§ ë…¸ë“œì— ì ìš©í•˜ì§€ ì•ŠìŒ (ì„ íƒì  ì‚¬ìš©)
- âœ… ê° ë…¸ë“œëŠ” ë…ë¦½ì ì¸ `get_llm()` í•¨ìˆ˜ ì‚¬ìš© ì¤‘

---

## ğŸ”§ ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš© (ë…¸ë“œ ê¸°ë³¸ ì„¤ì •)

```python
from app.domain.langgraph.utils.llm_factory import get_llm

# ë…¸ë“œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
llm = get_llm("intent_analyzer")
# â†’ temperature=0.3, model=DEFAULT_LLM_MODEL

llm = get_llm("turn_evaluator")
# â†’ temperature=0.1, model=DEFAULT_LLM_MODEL

llm = get_llm("writer")
# â†’ temperature=settings.LLM_TEMPERATURE, model=DEFAULT_LLM_MODEL
```

### ì»¤ìŠ¤í…€ ì„¤ì •

```python
# ì˜¨ë„ ë³€ê²½
llm = get_llm("writer", temperature=0.9)

# ìµœëŒ€ í† í° ìˆ˜ ì„¤ì •
llm = get_llm("writer", max_tokens=2000)

# ëª¨ë¸ ë³€ê²½
llm = get_llm("writer", model="gemini-2.0-flash-exp")
```

### ë‹¤ë¥¸ LLM íƒ€ì… ì‚¬ìš©

```python
# OpenAI ì‚¬ìš©
llm = get_llm("writer", llm_type="openai", model="gpt-4")

# Anthropic ì‚¬ìš© (êµ¬í˜„ í•„ìš”)
# llm = get_llm("writer", llm_type="anthropic", model="claude-3-opus-20240229")
```

---

## ğŸ“Š ë…¸ë“œë³„ ê¸°ë³¸ ì„¤ì •

### ì„¤ì • í…Œì´ë¸”

| ë…¸ë“œ ì´ë¦„ | LLM íƒ€ì… | Temperature | Model | Max Tokens |
|----------|---------|-------------|-------|------------|
| `intent_analyzer` | gemini | 0.3 | DEFAULT_LLM_MODEL | ê¸°ë³¸ê°’ |
| `writer` | gemini | settings.LLM_TEMPERATURE | DEFAULT_LLM_MODEL | settings.LLM_MAX_TOKENS |
| `turn_evaluator` | gemini | 0.1 | DEFAULT_LLM_MODEL | ê¸°ë³¸ê°’ |
| `holistic_evaluator` | gemini | 0.1 | DEFAULT_LLM_MODEL | ê¸°ë³¸ê°’ |
| `system_nodes` | gemini | 0.3 | DEFAULT_LLM_MODEL | ê¸°ë³¸ê°’ |

### ì„¤ì • ì •ì˜ ìœ„ì¹˜

```python
# app/domain/langgraph/utils/llm_factory.py
NODE_DEFAULT_CONFIGS: Dict[str, Dict[str, Any]] = {
    "intent_analyzer": {
        "llm_type": "gemini",
        "temperature": 0.3,
        "model": settings.DEFAULT_LLM_MODEL,
    },
    "writer": {
        "llm_type": "gemini",
        "temperature": getattr(settings, "LLM_TEMPERATURE", 0.7),
        "model": settings.DEFAULT_LLM_MODEL,
        "max_tokens": getattr(settings, "LLM_MAX_TOKENS", None),
    },
    # ... ê¸°íƒ€ ë…¸ë“œ ì„¤ì •
}
```

---

## ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### í˜„ì¬ ë°©ì‹ (ë…ë¦½ í•¨ìˆ˜)

```python
# turn_evaluator/utils.py
def get_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return ChatGoogleGenerativeAI(
        model=settings.DEFAULT_LLM_MODEL,
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.1,
    )
```

### ìƒˆë¡œìš´ ë°©ì‹ (Factory Pattern)

```python
# turn_evaluator/utils.py
from app.domain.langgraph.utils.llm_factory import get_llm as get_llm_factory

def get_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Factory Pattern ì‚¬ìš©)"""
    return get_llm_factory("turn_evaluator")
```

### ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜

1. **ë‹¨ê³„ 1**: Factory Pattern êµ¬í˜„ (ì™„ë£Œ)
2. **ë‹¨ê³„ 2**: í…ŒìŠ¤íŠ¸ ë…¸ë“œì— ì ìš©
3. **ë‹¨ê³„ 3**: ëª¨ë“  ë…¸ë“œì— ì ìš©
4. **ë‹¨ê³„ 4**: ê¸°ì¡´ ë…ë¦½ í•¨ìˆ˜ ì œê±°

---

## ğŸ¯ ì¥ë‹¨ì  ë¹„êµ

### í˜„ì¬ ë°©ì‹ (ë…ë¦½ í•¨ìˆ˜)

**ì¥ì **:
- âœ… ì½”ë“œê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
- âœ… ê° ë…¸ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬ë¨
- âœ… ë…¸ë“œë³„ë¡œ ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš© ê°€ëŠ¥
- âœ… ë³€ê²½ ì˜í–¥ ë²”ìœ„ê°€ ì‘ìŒ

**ë‹¨ì **:
- âš ï¸ ì—¬ëŸ¬ LLM íƒ€ì… ì „í™˜ ì‹œ ê° ë…¸ë“œ ìˆ˜ì • í•„ìš”
- âš ï¸ ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì • ê´€ë¦¬ ì–´ë ¤ì›€

### Factory Pattern

**ì¥ì **:
- âœ… ì—¬ëŸ¬ LLM íƒ€ì…ì„ ì‰½ê²Œ ì „í™˜ ê°€ëŠ¥
- âœ… ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì • ê´€ë¦¬
- âœ… ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš© (ì‹±ê¸€í†¤)
- âœ… í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°

**ë‹¨ì **:
- âš ï¸ ì½”ë“œ ë³€ê²½ í•„ìš” (ê° ë…¸ë“œì˜ `get_llm()` í˜¸ì¶œ ìˆ˜ì •)
- âš ï¸ ì„¤ì •ì´ ì¤‘ì•™ì— ì§‘ì¤‘ë˜ì–´ ë…¸ë“œë³„ ë…ë¦½ì„± ê°ì†Œ

---

## ğŸ“ ê¶Œì¥ ì‚¬í•­

### ì˜µì…˜ 1: í˜„ì¬ êµ¬ì¡° ìœ ì§€ (ê¶Œì¥)

**ì´ìœ **:
- ê° ë…¸ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬ë˜ì–´ ìœ ì§€ë³´ìˆ˜ê°€ ì‰¬ì›€
- ì½”ë“œê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
- ë…¸ë“œë³„ë¡œ ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš© ê°€ëŠ¥
- Factory Patternì€ í•„ìš” ì‹œì—ë§Œ ë„ì…

**ì ìš© ì‹œì **:
- ì—¬ëŸ¬ LLM íƒ€ì…ì„ ì‹¤ì œë¡œ ì‚¬ìš©í•´ì•¼ í•  ë•Œ
- ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì • ê´€ë¦¬ê°€ í•„ìš”í•  ë•Œ

### ì˜µì…˜ 2: Factory Patternìœ¼ë¡œ ì „í™˜

**ì´ìœ **:
- ì—¬ëŸ¬ LLM íƒ€ì…ì„ ì‰½ê²Œ ì „í™˜í•˜ê³  ì‹¶ì„ ë•Œ
- ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì • ê´€ë¦¬ê°€ í•„ìš”í•  ë•Œ
- ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©ì„ í†µí•œ ìµœì í™”

**ì ìš© ì‹œì **:
- í”„ë¡œì íŠ¸ ì´ˆê¸° ë‹¨ê³„
- ì—¬ëŸ¬ LLM íƒ€ì…ì„ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê¸° ì‹œì‘í•  ë•Œ

---

## ğŸ” êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### ìºì‹± ë©”ì»¤ë‹ˆì¦˜

```python
# LLM ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ (ë…¸ë“œë³„ + ì„¤ì •ë³„)
_llm_cache: Dict[str, Any] = {}

def _create_cache_key(node_name: str, llm_type: str, **kwargs) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    config_str = "_".join(f"{k}:{v}" for k, v in sorted(kwargs.items()) if v is not None)
    return f"{node_name}_{llm_type}_{config_str}"

def get_llm(node_name: str, **kwargs):
    """ë…¸ë“œë³„ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    cache_key = _create_cache_key(node_name, **final_config)
    
    # ìºì‹œì— ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    if cache_key in _llm_cache:
        return _llm_cache[cache_key]
    
    # ìƒˆ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ìºì‹±
    llm = _create_llm(**final_config)
    _llm_cache[cache_key] = llm
    return llm
```

### LLM íƒ€ì…ë³„ ìƒì„± í•¨ìˆ˜

```python
def _create_gemini_llm(**kwargs) -> ChatGoogleGenerativeAI:
    """Gemini LLM ìƒì„±"""
    return ChatGoogleGenerativeAI(
        model=kwargs.get("model", settings.DEFAULT_LLM_MODEL),
        google_api_key=kwargs.get("api_key", settings.GEMINI_API_KEY),
        temperature=kwargs.get("temperature", 0.3),
        max_output_tokens=kwargs.get("max_tokens"),
    )

def _create_openai_llm(**kwargs) -> ChatOpenAI:
    """OpenAI LLM ìƒì„±"""
    return ChatOpenAI(
        model=kwargs.get("model", "gpt-4"),
        api_key=kwargs.get("api_key", getattr(settings, "OPENAI_API_KEY", None)),
        temperature=kwargs.get("temperature", 0.3),
        max_tokens=kwargs.get("max_tokens"),
    )
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ìºì‹œ í™•ì¸

```python
from app.domain.langgraph.utils.llm_factory import get_llm, get_cache_info

# ì²« í˜¸ì¶œ
llm1 = get_llm("intent_analyzer")

# ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œì—ì„œ ì¬ì‚¬ìš©)
llm2 = get_llm("intent_analyzer")

# ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸
assert llm1 is llm2  # True

# ìºì‹œ ì •ë³´ í™•ì¸
cache_info = get_cache_info()
print(cache_info)  # {"cache_size": 1, "cached_keys": [...]}
```

### ìºì‹œ ì´ˆê¸°í™”

```python
from app.domain.langgraph.utils.llm_factory import clear_llm_cache

# ìºì‹œ ì´ˆê¸°í™” (í…ŒìŠ¤íŠ¸ìš©)
clear_llm_cache()
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [LLM Factory êµ¬í˜„](../app/domain/langgraph/utils/llm_factory.py)
- [LLM ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬ ê°€ì´ë“œ](./LLM_Instance_Management_Guide.md)



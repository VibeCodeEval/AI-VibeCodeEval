# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

LangGraph ë…¸ë“œì—ì„œ LLM í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ì„ ì •í™•í•˜ê²Œ ì¶”ì í•˜ê¸° ìœ„í•œ êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.

**í•µì‹¬ ë¬¸ì œ**: `with_structured_output`ì„ ì‚¬ìš©í•˜ë©´ Pydantic ëª¨ë¸ë§Œ ë°˜í™˜ë˜ì–´ ì›ë³¸ LLM ì‘ë‹µì˜ ë©”íƒ€ë°ì´í„°(í† í° ì‚¬ìš©ëŸ‰)ê°€ ì†ì‹¤ë©ë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**: Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLMì„ ë¨¼ì € í˜¸ì¶œí•˜ì—¬ í† í° ì‚¬ìš©ëŸ‰ì„ ì¶”ì¶œí•œ í›„, êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.

---

## ğŸ” ë¬¸ì œ ë¶„ì„

### `with_structured_output`ì˜ í•œê³„

```python
# âŒ ë¬¸ì œê°€ ë˜ëŠ” ì½”ë“œ
structured_llm = llm.with_structured_output(IntentClassification)
result = await structured_llm.ainvoke(messages)
# resultëŠ” Pydantic ëª¨ë¸ë§Œ ë°˜í™˜ â†’ ì›ë³¸ ì‘ë‹µ ë©”íƒ€ë°ì´í„° ì†ì‹¤
tokens = extract_token_usage(result)  # âŒ ì‹¤íŒ¨ (Pydantic ëª¨ë¸ì—ëŠ” ë©”íƒ€ë°ì´í„° ì—†ìŒ)
```

**ì›ì¸**:
- `with_structured_output`ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì›ë³¸ LLM ì‘ë‹µì„ Pydantic ëª¨ë¸ë¡œ ë³€í™˜
- ë³€í™˜ ê³¼ì •ì—ì„œ `usage_metadata`, `response_metadata` ë“±ì´ ì†ì‹¤ë¨
- ê²°ê³¼ì ìœ¼ë¡œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë¶ˆê°€

---

## âœ… í•´ê²° ë°©ë²•

### íŒ¨í„´ 1: ì›ë³¸ LLM ë¨¼ì € í˜¸ì¶œ (ê¶Œì¥)

**ì ìš© ìœ„ì¹˜**: `with_structured_output`ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ë…¸ë“œ

```python
async def intent_analysis(state: EvalTurnState) -> Dict[str, Any]:
    """ì˜ë„ ë¶„ì„ - í† í° ì¶”ì  ê°œì„  ë²„ì „"""
    
    llm = get_llm()
    structured_llm = llm.with_structured_output(IntentClassification)
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=prompt)
    ]
    
    # 1. ì›ë³¸ LLM í˜¸ì¶œ (í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œìš©)
    raw_response = await llm.ainvoke(messages)
    
    # 2. í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° Stateì— ëˆ„ì 
    tokens = extract_token_usage(raw_response)
    if tokens:
        accumulate_tokens(state, tokens, token_type="eval")
    
    # 3. êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ íŒŒì‹± (ì‹¤ì œ ì‚¬ìš©)
    parsed_response = await structured_llm.ainvoke(messages)
    
    return {
        "intent_types": [intent.value for intent in parsed_response.intent_types],
        "intent_confidence": parsed_response.confidence,
    }
```

**ì¥ì **:
- âœ… í† í° ì‚¬ìš©ëŸ‰ ì •í™•í•˜ê²Œ ì¶”ì¶œ ê°€ëŠ¥
- âœ… êµ¬ì¡°í™”ëœ ì¶œë ¥ ìœ ì§€
- âœ… ì½”ë“œê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€

**ë‹¨ì **:
- âš ï¸ LLMì„ ë‘ ë²ˆ í˜¸ì¶œ (ì›ë³¸ + êµ¬ì¡°í™”ëœ ì¶œë ¥)
- âš ï¸ ë¹„ìš©ì´ ì•½ê°„ ì¦ê°€ (í•˜ì§€ë§Œ í† í° ì¶”ì ì„ ìœ„í•´ í•„ìš”)

---

## ğŸ“ ì ìš©ëœ ë…¸ë“œ ëª©ë¡

### âœ… ì™„ë£Œëœ ë…¸ë“œ

#### 1. **2ë²ˆ ë…¸ë“œ: Intent Analyzer**
- **íŒŒì¼**: `app/domain/langgraph/nodes/intent_analyzer.py`
- **í•¨ìˆ˜**: `intent_analyzer()`
- **í† í° íƒ€ì…**: `chat`
- **ì ìš© ë°©ì‹**: Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLM í˜¸ì¶œ

```python
# Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLM í˜¸ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
formatted_messages = format_messages(prepare_input(chain_input))
raw_response = await llm.ainvoke(formatted_messages)

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° Stateì— ëˆ„ì 
tokens = extract_token_usage(raw_response)
if tokens:
    accumulate_tokens(state, tokens, token_type="chat")
```

#### 2. **4.0 ë…¸ë“œ: Intent Analysis (ì˜ë„ ë¶„ë¥˜)**
- **íŒŒì¼**: `app/domain/langgraph/nodes/turn_evaluator/analysis.py`
- **í•¨ìˆ˜**: `intent_analysis()`
- **í† í° íƒ€ì…**: `eval`
- **ì ìš© ë°©ì‹**: ì›ë³¸ LLM ë¨¼ì € í˜¸ì¶œ í›„ êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì‹±

```python
# ì›ë³¸ LLM ì‘ë‹µ ë°›ê¸° (í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œìš©)
raw_response = await llm.ainvoke(messages)

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° Stateì— ëˆ„ì 
tokens = extract_token_usage(raw_response)
if tokens:
    accumulate_tokens(state, tokens, token_type="eval")

# êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ íŒŒì‹±
parsed_response = await structured_llm.ainvoke(messages)
```

#### 3. **4ë²ˆ ë…¸ë“œ: Turn Evaluator (í‰ê°€ ì²´ì¸)**
- **íŒŒì¼**: `app/domain/langgraph/nodes/turn_evaluator/evaluators.py`
- **í•¨ìˆ˜**: `_evaluate_turn()`
- **í† í° íƒ€ì…**: `eval`
- **ì ìš© ë°©ì‹**: Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLM í˜¸ì¶œ

```python
# Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLM í˜¸ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
prepared_input = prepare_evaluation_input_internal(chain_input, eval_type, criteria)
formatted_messages = format_evaluation_messages(prepared_input)

# ì›ë³¸ LLM í˜¸ì¶œ (í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œìš©)
raw_response = await llm.ainvoke(formatted_messages)

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° Stateì— ëˆ„ì 
tokens = extract_token_usage(raw_response)
if tokens:
    accumulate_tokens(state, tokens, token_type="eval")
```

#### 4. **4.X ë…¸ë“œ: Answer Summary (ë‹µë³€ ìš”ì•½)**
- **íŒŒì¼**: `app/domain/langgraph/nodes/turn_evaluator/summary.py`
- **í•¨ìˆ˜**: `summarize_answer()`
- **í† í° íƒ€ì…**: `eval`
- **ì ìš© ë°©ì‹**: Chainì—ì„œ LLM ì‘ë‹µ ê°ì²´ ë³´ì¡´ (ì›ë³¸ LLM ì‚¬ìš©)

```python
# Chainì—ì„œ LLM ì‘ë‹µ ê°ì²´ ë³´ì¡´
summary_chain = (
    RunnableLambda(prepare_summary_input)
    | summary_prompt
    | get_llm()  # ì›ë³¸ LLM ì‚¬ìš© (with_structured_output ì—†ìŒ)
    | RunnableLambda(extract_summary_with_response)
)

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
llm_response = chain_result.get("_llm_response")
if llm_response:
    tokens = extract_token_usage(llm_response)
    if tokens:
        accumulate_tokens(state, tokens, token_type="eval")
```

#### 5. **6a ë…¸ë“œ: Holistic Flow Evaluator**
- **íŒŒì¼**: `app/domain/langgraph/nodes/holistic_evaluator/flow.py`
- **í•¨ìˆ˜**: `_eval_holistic_flow_impl()`
- **í† í° íƒ€ì…**: `eval`
- **ì ìš© ë°©ì‹**: Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLM í˜¸ì¶œ

```python
# Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLM í˜¸ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
prepared_input = prepare_holistic_input(chain_input)
formatted_messages = format_holistic_messages(prepared_input)

# ì›ë³¸ LLM í˜¸ì¶œ (í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œìš©)
raw_response = await llm.ainvoke(formatted_messages)

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° Stateì— ëˆ„ì 
tokens = extract_token_usage(raw_response)
if tokens:
    accumulate_tokens(state, tokens, token_type="eval")
```

#### 6. **6c ë…¸ë“œ: Code Performance Evaluator**
- **íŒŒì¼**: `app/domain/langgraph/nodes/holistic_evaluator/performance.py`
- **í•¨ìˆ˜**: `_eval_code_performance_impl()`
- **í† í° íƒ€ì…**: `eval`
- **ì ìš© ë°©ì‹**: Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLM í˜¸ì¶œ

#### 7. **6d ë…¸ë“œ: Code Correctness Evaluator**
- **íŒŒì¼**: `app/domain/langgraph/nodes/holistic_evaluator/correctness.py`
- **í•¨ìˆ˜**: `_eval_code_correctness_impl()`
- **í† í° íƒ€ì…**: `eval`
- **ì ìš© ë°©ì‹**: Chain ì‹¤í–‰ ì „ì— ì›ë³¸ LLM í˜¸ì¶œ

#### 8. **3ë²ˆ ë…¸ë“œ: Writer LLM**
- **íŒŒì¼**: `app/domain/langgraph/nodes/writer.py`
- **í•¨ìˆ˜**: `writer_llm()`
- **í† í° íƒ€ì…**: `chat`
- **ì ìš© ë°©ì‹**: Chainì—ì„œ LLM ì‘ë‹µ ê°ì²´ ë³´ì¡´ (ì›ë³¸ LLM ì‚¬ìš©)

```python
# Chainì—ì„œ LLM ì‘ë‹µ ê°ì²´ ë³´ì¡´
_base_writer_chain = (
    RunnableLambda(prepare_writer_input)
    | RunnableLambda(format_writer_messages)
    | _writer_llm  # ì›ë³¸ LLM ì‚¬ìš©
    | RunnableLambda(extract_content_with_response)  # ì‘ë‹µ ê°ì²´ ë³´ì¡´
)

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
llm_response = chain_result.get("_llm_response")
if llm_response:
    tokens = extract_token_usage(llm_response)
    if tokens:
        accumulate_tokens(state, tokens, token_type="chat")
```

---

## ğŸ”§ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### í† í° ì¶”ì  ìœ í‹¸ë¦¬í‹°

**íŒŒì¼**: `app/domain/langgraph/utils/token_tracking.py`

#### `extract_token_usage(response)`
- LLM ì‘ë‹µì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
- Gemini API í˜•ì‹ ì§€ì›: `usage_metadata.input_tokens`, `usage_metadata.output_tokens`
- ë‹¤ë¥¸ LLM í˜•ì‹ë„ ì§€ì› (OpenAI ë“±)

#### `accumulate_tokens(state, new_tokens, token_type)`
- Stateì— í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
- `token_type`: `"chat"` ë˜ëŠ” `"eval"`
- Stateì˜ `chat_tokens` ë˜ëŠ” `eval_tokens` í•„ë“œì— ëˆ„ì 

#### `get_token_summary(state)`
- Stateì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ë°˜í™˜
- `{"chat_tokens": {...}, "eval_tokens": {...}}` í˜•ì‹

---

## ğŸ“Š í† í° íƒ€ì… ë¶„ë¥˜

### `chat` íƒ€ì… (ì±„íŒ… ê²€ì‚¬)
- **2ë²ˆ ë…¸ë“œ**: Intent Analyzer
- **3ë²ˆ ë…¸ë“œ**: Writer LLM

### `eval` íƒ€ì… (í‰ê°€)
- **4ë²ˆ ë…¸ë“œ**: Turn Evaluator (ì˜ë„ ë¶„ì„, í‰ê°€, ìš”ì•½)
- **6ë²ˆ ë…¸ë“œ**: Holistic Evaluator (í”Œë¡œìš°, ì„±ëŠ¥, ì •í™•ì„±)

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. LLM ì´ì¤‘ í˜¸ì¶œ
- `with_structured_output` ì‚¬ìš© ì‹œ ì›ë³¸ LLMì„ ë¨¼ì € í˜¸ì¶œí•˜ë©´ LLMì´ ë‘ ë²ˆ í˜¸ì¶œë¨
- ë¹„ìš©ì´ ì•½ê°„ ì¦ê°€í•˜ì§€ë§Œ, í† í° ì¶”ì ì„ ìœ„í•´ í•„ìš”
- í–¥í›„ LangChainì—ì„œ ë©”íƒ€ë°ì´í„° ë³´ì¡´ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ë©´ ê°œì„  ê°€ëŠ¥

### 2. ì—ëŸ¬ ì²˜ë¦¬
- ì›ë³¸ LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ êµ¬ì¡°í™”ëœ ì¶œë ¥ë„ ì‹¤íŒ¨í•  ê°€ëŠ¥ì„± ë†’ìŒ
- í† í° ì¶”ì  ì‹¤íŒ¨ëŠ” ê²½ê³  ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰

### 3. ì„±ëŠ¥ ì˜í–¥
- LLM í˜¸ì¶œì´ ë‘ ë²ˆ ë°œìƒí•˜ë¯€ë¡œ ì‘ë‹µ ì‹œê°„ì´ ì•½ê°„ ì¦ê°€
- í•˜ì§€ë§Œ í† í° ì¶”ì ì˜ ì •í™•ì„±ì„ ìœ„í•´ í•„ìš”

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í† í° ì¶”ì  í™•ì¸ ë°©ë²•

```python
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸: test_token_tracking.py
def test_chat_with_tokens():
    """ì¼ë°˜ ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡ ë° í† í° ì‚¬ìš©ëŸ‰ í™•ì¸"""
    response = requests.post("/api/chat/message", json={
        "session_id": "test-session",
        "message": "DPì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"
    })
    
    result = response.json()
    
    # í† í° ì‚¬ìš©ëŸ‰ í™•ì¸
    if "chat_tokens" in result:
        print(f"Chat tokens: {result['chat_tokens']}")
    
    if "eval_tokens" in result:
        print(f"Eval tokens: {result['eval_tokens']}")
```

---

## ğŸ“ˆ í–¥í›„ ê°œì„  ë°©í–¥

### 1. LangChain ê°œì„  ëŒ€ì‘
- LangChainì—ì„œ `with_structured_output`ì´ ë©”íƒ€ë°ì´í„°ë¥¼ ë³´ì¡´í•˜ë„ë¡ ê°œì„ ë˜ë©´
- ì›ë³¸ LLM í˜¸ì¶œ ì—†ì´ í† í° ì¶”ì  ê°€ëŠ¥

### 2. ìºì‹± ìµœì í™”
- ë™ì¼í•œ ë©”ì‹œì§€ì— ëŒ€í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ê²°ê³¼ë¥¼ ìºì‹±
- ì›ë³¸ LLM í˜¸ì¶œ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©

### 3. ë¹„ë™ê¸° ìµœì í™”
- ì›ë³¸ LLM í˜¸ì¶œê³¼ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ë³‘ë ¬ ì²˜ë¦¬ (ë¶ˆê°€ëŠ¥ - êµ¬ì¡°í™”ëœ ì¶œë ¥ì´ ì›ë³¸ ì‘ë‹µ í•„ìš”)
- ëŒ€ì‹  ì›ë³¸ ì‘ë‹µì„ ì¬ì‚¬ìš©í•˜ëŠ” ë°©ì‹ ê³ ë ¤

---

## ğŸ“ ì°¸ê³  ìë£Œ

- [LangChain Structured Output](https://python.langchain.com/docs/modules/model_io/output_parsers/structured)
- [Gemini API Usage Metadata](https://ai.google.dev/api/generate-content#usage-metadata)
- [Token Tracking Utility](../app/domain/langgraph/utils/token_tracking.py)



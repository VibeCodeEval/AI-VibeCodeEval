# LLM ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬ ê°€ì´ë“œ

## ğŸ“‹ ì§ˆë¬¸ 1: "LLM ì—¬ëŸ¬ë²ˆ í˜¸ì¶œ"ì´ ë¬´ìŠ¨ ëœ»ì¸ê°€?

### í˜„ì¬ ìƒí™© ë¶„ì„

#### âŒ ë¬¸ì œê°€ ë˜ëŠ” ì½”ë“œ (ì´ì „)
```python
# writer.py - ë§¤ë²ˆ ìƒˆ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
def create_writer_chain():
    llm = get_llm()  # âš ï¸ ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±!
    chain = (...)
    return chain

async def writer_llm(state: MainGraphState):
    chain = create_writer_chain()  # âš ï¸ ë§¤ë²ˆ í˜¸ì¶œë  ë•Œë§ˆë‹¤ ìƒˆ Chain ìƒì„±
    ai_content = await chain.ainvoke(state)
```

**ë¬¸ì œì :**
- `writer_llm()` í•¨ìˆ˜ê°€ í˜¸ì¶œë  ë•Œë§ˆë‹¤:
  1. `create_writer_chain()` ì‹¤í–‰
  2. `get_llm()` ì‹¤í–‰ â†’ **ìƒˆ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±**
  3. Chain ìƒì„±
  4. LLM í˜¸ì¶œ

**ì˜í–¥:**
- LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì˜¤ë²„í—¤ë“œ (ë¯¸ë¯¸í•˜ì§€ë§Œ ëˆ„ì  ê°€ëŠ¥)
- ë©”ëª¨ë¦¬ì— ë¶ˆí•„ìš”í•œ ì¸ìŠ¤í„´ìŠ¤ê°€ ìŒ“ì¼ ìˆ˜ ìˆìŒ

#### âœ… ê°œì„ ëœ ì½”ë“œ (í˜„ì¬)
```python
# writer.py - ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ìºì‹±
_writer_chain = None
_writer_llm = None

def get_writer_chain():
    global _writer_chain, _writer_llm
    if _writer_chain is None:
        _writer_llm = get_llm()  # âœ… 1ë²ˆë§Œ ìƒì„±
        _writer_chain = (...)
    return _writer_chain  # âœ… ì¬ì‚¬ìš©

async def writer_llm(state: MainGraphState):
    chain = get_writer_chain()  # âœ… ìºì‹±ëœ Chain ì¬ì‚¬ìš©
    ai_content = await chain.ainvoke(state)
```

**ê°œì„ ì :**
- ì²« í˜¸ì¶œ ì‹œì—ë§Œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- ì´í›„ í˜¸ì¶œì—ì„œëŠ” ìºì‹±ëœ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©

---

## ğŸ“‹ ì§ˆë¬¸ 2: ì‹±ê¸€í†¤ìœ¼ë¡œ í•˜ë©´ ì§€ì†ì ìœ¼ë¡œ ì—°ê²°í•´ì„œ ì‚¬ìš©í•œë‹¤ëŠ” ëœ»ì¸ê°€?

### âŒ ì˜¤í•´: "ì—°ê²° ìœ ì§€"ê°€ ì•„ë‹˜

**ì¤‘ìš”í•œ ì :**
- LLM ì¸ìŠ¤í„´ìŠ¤ëŠ” **"ì—°ê²°"ì„ ìœ ì§€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**
- ê° API í˜¸ì¶œì€ **ë…ë¦½ì ì¸ HTTP ìš”ì²­**ì…ë‹ˆë‹¤
- ì‹±ê¸€í†¤ íŒ¨í„´ì€ **ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤

### âœ… ì‹¤ì œ ì˜ë¯¸

```python
# ì‹±ê¸€í†¤ íŒ¨í„´ì˜ ì˜ë¯¸
_llm_instance = None

def get_llm():
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = ChatGoogleGenerativeAI(...)  # âœ… 1ë²ˆë§Œ ìƒì„±
    return _llm_instance  # âœ… ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

# ì‚¬ìš© ì˜ˆì‹œ
llm1 = get_llm()  # ì²« í˜¸ì¶œ: ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm2 = get_llm()  # ë‘ ë²ˆì§¸ í˜¸ì¶œ: ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
# llm1 is llm2  # True (ê°™ì€ ê°ì²´)

# ê° API í˜¸ì¶œì€ ì—¬ì „íˆ ë…ë¦½ì 
response1 = await llm1.ainvoke("Hello")  # HTTP ìš”ì²­ 1
response2 = await llm2.ainvoke("World")  # HTTP ìš”ì²­ 2 (ë…ë¦½ì )
```

**í•µì‹¬:**
- âœ… **ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©**: ê°™ì€ ì„¤ì •ì˜ LLM ê°ì²´ë¥¼ ì¬ì‚¬ìš©
- âŒ **ì—°ê²° ìœ ì§€ ì•„ë‹˜**: ê° API í˜¸ì¶œì€ ë…ë¦½ì ì¸ HTTP ìš”ì²­
- âœ… **ë©”ëª¨ë¦¬ ì ˆì•½**: ë¶ˆí•„ìš”í•œ ê°ì²´ ìƒì„± ë°©ì§€
- âœ… **ì´ˆê¸°í™” ì˜¤ë²„í—¤ë“œ ê°ì†Œ**: ì„¤ì • íŒŒì‹± ë“± 1ë²ˆë§Œ ìˆ˜í–‰

---

## ğŸ“‹ ì§ˆë¬¸ 3: ë…¸ë“œë³„ë¡œ ë‹¤ë¥¸ LLM ì„¤ì •ì„ ì‚¬ìš©í•  ì˜ˆì •ì¸ë°, ì‹±ê¸€í†¤ì´ ë³„ë¡œì¸ê°€?

### í˜„ì¬ ë…¸ë“œë³„ LLM ì„¤ì • ë¶„ì„

| ë…¸ë“œ | Temperature | Model | Max Tokens |
|------|-------------|-------|------------|
| **Intent Analyzer** | 0.3 | DEFAULT_LLM_MODEL | ê¸°ë³¸ê°’ |
| **Writer** | settings.LLM_TEMPERATURE | DEFAULT_LLM_MODEL | settings.LLM_MAX_TOKENS |
| **Turn Evaluator** | 0.1 | DEFAULT_LLM_MODEL | ê¸°ë³¸ê°’ |
| **Holistic Evaluator** | 0.1 | DEFAULT_LLM_MODEL | ê¸°ë³¸ê°’ |
| **System Nodes** | 0.3 | DEFAULT_LLM_MODEL | ê¸°ë³¸ê°’ |

### âš ï¸ ë¬¸ì œì : ë‹¨ì¼ ì‹±ê¸€í†¤ì€ ë¶€ì í•©

**ë§Œì•½ ë‹¨ì¼ ì‹±ê¸€í†¤ì„ ì‚¬ìš©í•œë‹¤ë©´:**
```python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ
_llm_instance = None

def get_llm():
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = ChatGoogleGenerativeAI(temperature=0.3)  # ê³ ì •ê°’
    return _llm_instance

# ë¬¸ì œ: ëª¨ë“  ë…¸ë“œê°€ ê°™ì€ temperatureë¥¼ ì‚¬ìš©í•˜ê²Œ ë¨
# - Intent Analyzer: 0.3 âœ…
# - Turn Evaluator: 0.1ì´ì–´ì•¼ í•˜ëŠ”ë° 0.3 ì‚¬ìš© âŒ
```

### âœ… í•´ê²° ë°©ì•ˆ: ë…¸ë“œë³„ ì‹±ê¸€í†¤ (Factory Pattern)

```python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ë…¸ë“œë³„ë¡œ ë‹¤ë¥¸ ì„¤ì •ì˜ LLM ì¸ìŠ¤í„´ìŠ¤ ìºì‹±
_llm_cache = {}

def get_llm(node_name: str, temperature: float = None, model: str = None):
    """
    ë…¸ë“œë³„ë¡œ ë‹¤ë¥¸ ì„¤ì •ì˜ LLM ì¸ìŠ¤í„´ìŠ¤ ìºì‹±
    
    Args:
        node_name: ë…¸ë“œ ì´ë¦„ (ì˜ˆ: "intent_analyzer", "writer", "turn_evaluator")
        temperature: ì˜¨ë„ ì„¤ì • (ë…¸ë“œë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        model: ëª¨ë¸ ì´ë¦„ (ë…¸ë“œë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    
    Returns:
        ìºì‹±ëœ LLM ì¸ìŠ¤í„´ìŠ¤
    """
    # ìºì‹œ í‚¤ ìƒì„± (ë…¸ë“œëª… + ì„¤ì • ì¡°í•©)
    cache_key = f"{node_name}_{temperature}_{model}"
    
    if cache_key not in _llm_cache:
        _llm_cache[cache_key] = ChatGoogleGenerativeAI(
            model=model or settings.DEFAULT_LLM_MODEL,
            google_api_key=settings.GEMINI_API_KEY,
            temperature=temperature or 0.3,
        )
    
    return _llm_cache[cache_key]

# ì‚¬ìš© ì˜ˆì‹œ
# Intent Analyzer
intent_llm = get_llm("intent_analyzer", temperature=0.3)

# Turn Evaluator
eval_llm = get_llm("turn_evaluator", temperature=0.1)

# Writer
writer_llm = get_llm("writer", temperature=settings.LLM_TEMPERATURE)
```

### ğŸ¯ ê¶Œì¥ êµ¬ì¡°: ë…¸ë“œë³„ ëª¨ë“ˆ ë ˆë²¨ ìºì‹± (í˜„ì¬ êµ¬ì¡° ìœ ì§€)

**í˜„ì¬ êµ¬ì¡°ê°€ ì´ë¯¸ ì í•©í•©ë‹ˆë‹¤:**

```python
# âœ… intent_analyzer.py - ë…¸ë“œë³„ë¡œ ë…ë¦½ì ì¸ ìºì‹±
llm = get_llm()  # temperature=0.3
structured_llm = llm.with_structured_output(IntentAnalysisResult)
intent_analysis_chain = (...)

# âœ… turn_evaluator/utils.py - ë…¸ë“œë³„ë¡œ ë…ë¦½ì ì¸ ìºì‹±
def get_llm():
    return ChatGoogleGenerativeAI(temperature=0.1)  # ë‹¤ë¥¸ ì„¤ì •

# âœ… writer.py - ë…¸ë“œë³„ë¡œ ë…ë¦½ì ì¸ ìºì‹±
_writer_llm = None
def get_writer_chain():
    global _writer_llm
    if _writer_llm is None:
        _writer_llm = get_llm()  # temperature=settings.LLM_TEMPERATURE
    return _writer_chain
```

**ì¥ì :**
- âœ… ê° ë…¸ë“œê°€ ë…ë¦½ì ì¸ ì„¤ì • ì‚¬ìš© ê°€ëŠ¥
- âœ… ë…¸ë“œë³„ë¡œ ìºì‹±ë˜ì–´ ë¶ˆí•„ìš”í•œ ì¬ìƒì„± ë°©ì§€
- âœ… ì½”ë“œê°€ ëª…í™•í•˜ê³  ìœ ì§€ë³´ìˆ˜ ìš©ì´

**ë‹¨ì :**
- âš ï¸ ë…¸ë“œê°€ ë§ì•„ì§€ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ (í•˜ì§€ë§Œ ë¯¸ë¯¸í•¨)

---

## ğŸ“Š ìµœì¢… ê¶Œì¥ ì‚¬í•­

### í˜„ì¬ êµ¬ì¡° ìœ ì§€ (ë…¸ë“œë³„ ëª¨ë“ˆ ë ˆë²¨ ìºì‹±)

**ì´ìœ :**
1. âœ… ë…¸ë“œë³„ë¡œ ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš© ê°€ëŠ¥
2. âœ… ê° ë…¸ë“œ ë‚´ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©
3. âœ… ì½”ë“œê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
4. âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ëŠ” ë¯¸ë¯¸í•¨ (ë…¸ë“œë‹¹ 1ê°œ ì¸ìŠ¤í„´ìŠ¤)

**í˜„ì¬ êµ¬ì¡° ì˜ˆì‹œ:**
```python
# intent_analyzer.py
llm = get_llm()  # temperature=0.3, ëª¨ë“ˆ ë ˆë²¨ì—ì„œ 1ë²ˆë§Œ ìƒì„±
intent_analysis_chain = (...)  # ëª¨ë“ˆ ë ˆë²¨ì—ì„œ 1ë²ˆë§Œ ìƒì„±

# turn_evaluator/utils.py
def get_llm():
    return ChatGoogleGenerativeAI(temperature=0.1)  # ë…¸ë“œë³„ ë…ë¦½ ì„¤ì •

# writer.py
_writer_chain = None
_writer_llm = None
def get_writer_chain():
    global _writer_chain, _writer_llm
    if _writer_chain is None:
        _writer_llm = get_llm()  # temperature=settings.LLM_TEMPERATURE
        _writer_chain = (...)
    return _writer_chain
```

---

## ğŸ” LangSmith Tracing ë° Middleware ê³„íš

### LangSmith Tracing (6ë²ˆ Nodeì— ì¶”ê°€ ì˜ˆì •)

**ì¶”ê°€ ìœ„ì¹˜:**
- `app/domain/langgraph/nodes/holistic_evaluator/flow.py` (6a)
- `app/domain/langgraph/nodes/holistic_evaluator/performance.py` (6c)
- `app/domain/langgraph/nodes/holistic_evaluator/correctness.py` (6d)

**êµ¬í˜„ ë°©ë²•:**
```python
from langsmith import traceable

@traceable(name="eval_holistic_flow")
async def eval_holistic_flow(state: MainGraphState):
    # Chain ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ LangSmithì— ì¶”ì 
    result = await holistic_chain.ainvoke({"structured_logs": structured_logs})
    return result
```

**ë˜ëŠ” Chain ë ˆë²¨ì—ì„œ:**
```python
from langchain_core.runnables import RunnableConfig
from langsmith import traceable

# Chainì— LangSmith í†µí•©
holistic_chain = (
    RunnableLambda(prepare_holistic_input)
    | RunnableLambda(format_holistic_messages)
    | structured_llm
    | RunnableLambda(process_holistic_output)
).with_config({"callbacks": [LangSmithTracer()]})
```

### Middleware ë„ì… (ë¹ ë¥¸ ì‹œì¼ ë‚´ ì˜ˆì •)

**ì¶”ê°€í•  Middleware:**
1. **ì¬ì‹œë„ (Retry)**: Rate Limit, Timeout ì‹œ ìë™ ì¬ì‹œë„
2. **Rate Limiting**: API í˜¸ì¶œ ë¹ˆë„ ì œí•œ
3. **ë¡œê¹…**: ëª¨ë“  LLM í˜¸ì¶œ ë¡œê¹…
4. **ì—ëŸ¬ ì²˜ë¦¬**: í†µì¼ëœ ì—ëŸ¬ ì²˜ë¦¬

**êµ¬í˜„ ë°©ë²•:**
```python
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.middleware import RunnableMiddleware

# ì¬ì‹œë„ Middleware
class RetryMiddleware(RunnableMiddleware):
    async def ainvoke(self, input, config=None, **kwargs):
        max_retries = 3
        for attempt in range(max_retries):
            try:
                return await super().ainvoke(input, config, **kwargs)
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)  # Exponential backoff

# Chainì— Middleware ì ìš©
holistic_chain = (
    RetryMiddleware() |
    RunnableLambda(prepare_holistic_input)
    | RunnableLambda(format_holistic_messages)
    | structured_llm
    | RunnableLambda(process_holistic_output)
)
```

---

## ğŸ“ ìš”ì•½

### 1. "LLM ì—¬ëŸ¬ë²ˆ í˜¸ì¶œ"ì˜ ì˜ë¯¸
- âŒ ë¬¸ì œ: ë§¤ë²ˆ ìƒˆ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- âœ… í•´ê²°: ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ìºì‹± (í˜„ì¬ ì™„ë£Œ)

### 2. ì‹±ê¸€í†¤ íŒ¨í„´ì˜ ì˜ë¯¸
- âŒ ì˜¤í•´: "ì—°ê²° ìœ ì§€"ê°€ ì•„ë‹˜
- âœ… ì‹¤ì œ: ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš© (ê°™ì€ ì„¤ì •ì˜ ê°ì²´ë¥¼ ì¬ì‚¬ìš©)

### 3. ë…¸ë“œë³„ ë‹¤ë¥¸ LLM ì„¤ì •
- âœ… í˜„ì¬ êµ¬ì¡°ê°€ ì í•©: ë…¸ë“œë³„ ëª¨ë“ˆ ë ˆë²¨ ìºì‹±
- âœ… ê° ë…¸ë“œê°€ ë…ë¦½ì ì¸ ì„¤ì • ì‚¬ìš© ê°€ëŠ¥
- âœ… ë¶ˆí•„ìš”í•œ ì¬ìƒì„± ë°©ì§€

### 4. LangSmith & Middleware ê³„íš
- âœ… LangSmith: 6ë²ˆ Nodeì— ì¶”ê°€ ì˜ˆì •
- âœ… Middleware: ë¹ ë¥¸ ì‹œì¼ ë‚´ ë„ì… ì˜ˆì •

---

**ê²°ë¡ **: í˜„ì¬ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë˜, LangSmith Tracingê³¼ Middlewareë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ìµœì ì˜ ë°©ì•ˆì…ë‹ˆë‹¤.

